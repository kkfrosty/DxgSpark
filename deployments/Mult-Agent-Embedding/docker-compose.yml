#
# Minimal multi-agent embedding stack: GPT-OSS-120B MXFP4 + Qwen3 embeddings
#
services:
  gpt-oss-120b:
    image: vllm/vllm-openai:latest
    container_name: gpt-oss-120b
    ipc: host
    restart: unless-stopped
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - TORCH_CUDA_ARCH_LIST=121
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - /home/kfrost/assets/models:/models:ro
    ports:
      - "8000:8000"
    command: >
      --model /models/gpt-oss-120b-mxfp4
      --tensor-parallel-size 1
      --max-model-len 65536
      --gpu-memory-utilization 0.80
      --max-num-seqs 2
      --port 8000
    --enforce-eager

  qwen3-embedding:
    build:
      context: /home/kfrost/dgx-spark-playbooks/nvidia/multi-agent-chatbot/assets
      dockerfile: Dockerfile.llamacpp
    image: local/llama.cpp:server-cuda
    container_name: qwen3-embedding
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - /home/kfrost/assets/models:/models:ro
    ports:
      - "8001:8000"
    command:
      - "-m"
      - "/models/Qwen3-Embedding-4B-Q8_0.gguf"
      - "--port"
      - "8000"
      - "--host"
      - "0.0.0.0"
      - "--jinja"
      - "--embeddings"

networks:
  default:
    name: chatbot-net
