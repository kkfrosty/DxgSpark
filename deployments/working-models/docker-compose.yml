#
# Unified Model Deployment - NVIDIA NIM
# All models accessible through a single, consistent interface
# Users can load/unload models via the benchMark UI
#
# Port assignments:
#   8000 - GPT-OSS-20B (when loaded)
#   8010 - GPT-OSS-120B (when loaded)
#   8001 - BGE Embeddings (shared)
#

services:
  # GPT-OSS-20B - Mid-size reasoning model
  gpt-oss-20b:
    image: nvcr.io/nim/openai/gpt-oss-20b:latest
    container_name: gpt-oss-20b-nim
    shm_size: '16GB'
    stdin_open: true
    tty: true
    ports:
      - "8000:8000"
    volumes:
      - /home/kfrost/assets/models:/opt/nim/.cache
    user: "${UID:-1000}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
    restart: "no"  # Manual control via benchMark UI
    profiles:
      - manual  # Don't start automatically
    networks:
      - model-net

  # GPT-OSS-120B - Large reasoning model
  gpt-oss-120b:
    image: nvcr.io/nim/openai/gpt-oss-120b:latest
    container_name: gpt-oss-120b-nim
    shm_size: '16GB'
    stdin_open: true
    tty: true
    ports:
      - "8010:8000"
    volumes:
      - /home/kfrost/assets/models:/opt/nim/.cache
    user: "${UID:-1000}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
      - NIM_MODEL_PROFILE=e2f00b2cbfb168f907c8d6d4d40406f7261111fbab8b3417a485dcd19d10cc98
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
    restart: "no"  # Manual control via benchMark UI
    profiles:
      - manual  # Don't start automatically
    networks:
      - model-net

  # BGE Embeddings - Shared embedding service
  bge-embedding:
    image: nvcr.io/nim/baai/bge-m3:latest
    container_name: bge-embedding-nim
    shm_size: '8GB'
    stdin_open: true
    tty: true
    ports:
      - "8001:8000"
    volumes:
      - /home/kfrost/assets/models:/opt/nim/.cache
    user: "${UID:-1000}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]  # Use specific GPU to avoid conflicts
              capabilities: [gpu]
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - model-net

networks:
  model-net:
    name: unified-model-net
    driver: bridge
